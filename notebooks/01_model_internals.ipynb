{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 01: GPT-Neo-125M — Model Internals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl4CQsDdcfu-"
      },
      "source": [
        "#### 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Yb-r-duetJKd"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "import warnings\n",
        "import yaml\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ImkZ5ghjcVcK"
      },
      "outputs": [],
      "source": [
        "# Setting random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qw9INwncrPp"
      },
      "source": [
        "#### 2. Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"../config.yaml\", \"r\") as f:\n",
        "    config = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ClMoevMXtOxP"
      },
      "outputs": [],
      "source": [
        "# Starting with a smaller model for quicker iteration\n",
        "model=config['model']['name']\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "UJ5qIikettJo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Loading weights: 100%|██████████| 160/160 [00:00<00:00, 548.16it/s, Materializing param=transformer.wte.weight]                         \n",
            "\u001b[1mGPTNeoForCausalLM LOAD REPORT\u001b[0m from: EleutherAI/gpt-neo-125M\n",
            "Key                                                   | Status     |  | \n",
            "------------------------------------------------------+------------+--+-\n",
            "transformer.h.{0, 2, 4, 6, 8, 10}.attn.attention.bias | UNEXPECTED |  | \n",
            "transformer.h.{0...11}.attn.attention.masked_bias     | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPTNeoForCausalLM(\n",
              "  (transformer): GPTNeoModel(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(2048, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPTNeoBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTNeoAttention(\n",
              "          (attention): GPTNeoSelfAttention(\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPTNeoMLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# loading the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model,\n",
        "    output_attentions=True,\n",
        "    output_hidden_states=True,\n",
        ").to(device)\n",
        "# Setting eval model\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm9v8EYhdCsZ"
      },
      "source": [
        "#### 3. Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI8ocs7pdHgt",
        "outputId": "a03b1824-ca9f-4e40-a32b-3e9e39a16111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Architecture:\n",
            "  Model type: gpt_neo\n",
            "  Number of layers: 12\n",
            "  Number of attention heads: 12\n",
            "  Hidden size: 768\n",
            "  Vocabulary size: 50257\n",
            "  Max position embeddings: 2048\n",
            "\n",
            "Total parameters: 125,198,592\n"
          ]
        }
      ],
      "source": [
        "model_config = model.config\n",
        "print(\"Model Architecture:\")\n",
        "print(f\"  Model type: {model_config.model_type}\")\n",
        "print(f\"  Number of layers: {model_config.num_layers}\")\n",
        "print(f\"  Number of attention heads: {model_config.num_heads}\")\n",
        "print(f\"  Hidden size: {model_config.hidden_size}\")\n",
        "print(f\"  Vocabulary size: {model_config.vocab_size}\")\n",
        "print(f\"  Max position embeddings: {model_config.max_position_embeddings}\")\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtuDmwODdMoB"
      },
      "source": [
        "#### 4. Generating output for a test prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample prompt\n",
        "prompt=config['analysis']['prompt']\n",
        "# Tokenizing prompt into tensor for the model\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NjNl8KXt0BP",
        "outputId": "d25776e4-44f0-4688-8ebe-70e3629b62c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt tokens: 15\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nPrompt tokens: {len(inputs['input_ids'][0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHUEv5d1t_uj",
        "outputId": "822c4e80-ffab-4828-8fe7-ffddc99bd6aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=config['inference']['max_new_tokens'],\n",
        "    do_sample=config['inference']['do_sample'],\n",
        "    repetition_penalty=config['inference']['repetition_penalty'],\n",
        "    return_dict_in_generate=True,\n",
        "    output_attentions=True,\n",
        "    output_hidden_states=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncVYGECFuCXY",
        "outputId": "91ffbb22-4f0e-4aea-c838-df1e3fdb66f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text:\n",
            "\n",
            "Marie Curie was a physicist who discovered radium. She was born in the United States and moved to Canada when she was eight years old, where she studied physics at the University of Toronto. She became interested in nuclear fusion theory after her father died.\n",
            "\n",
            "Curie\n"
          ]
        }
      ],
      "source": [
        "# Decoding the generated text\n",
        "decoded = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
        "print(\"Generated text:\\n\")\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfpcUoL4dhdD",
        "outputId": "561ea3e5-75fc-4622-d4da-b19ebfc18294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New tokens: 40\n"
          ]
        }
      ],
      "source": [
        "new_tokens = output.sequences[0][len(inputs['input_ids'][0]):]\n",
        "print(f\"New tokens: {len(new_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers: 12\n",
            "Hidden size: 768\n",
            "Generation:\n",
            "Prompt tokens: 15\n",
            "Generated tokens: 40\n",
            "Total generation steps: 40\n",
            "Captured Internals:\n",
            "Attention tensors per step: 12 layers\n",
            "Hidden state tensors per step: 13 layers\n",
            "Attention shape: torch.Size([1, 12, 15, 15])\n",
            "Hidden state shape: torch.Size([1, 15, 768])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Layers: {model_config.num_layers}\")\n",
        "print(f\"Hidden size: {model_config.hidden_size}\")\n",
        "print(f\"Generation:\")\n",
        "print(f\"Prompt tokens: {len(inputs['input_ids'][0])}\")\n",
        "print(f\"Generated tokens: {len(new_tokens)}\")\n",
        "print(f\"Total generation steps: {len(output.attentions)}\")\n",
        "print(f\"Captured Internals:\")\n",
        "print(f\"Attention tensors per step: {len(output.attentions[0])} layers\")\n",
        "print(f\"Hidden state tensors per step: {len(output.hidden_states[0])} layers\")\n",
        "print(f\"Attention shape: {output.attentions[0][0].shape}\")\n",
        "print(f\"Hidden state shape: {output.hidden_states[0][0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci2vFrR6hDz7"
      },
      "source": [
        "- GPT-Neo-125M: 12 layers, 12 attention heads, 768 hidden dimensions, ~125M parameters\n",
        "- Prompt tokenizes to 15 tokens, model generated 40 new tokens\n",
        "- For each generation step we capture:\n",
        "  - 12 attention tensors shaped [batch, heads, seq_len, seq_len]\n",
        "  - 13 hidden state tensors (embedding + 12 layers) shaped [batch, seq_len, hidden_size]\n",
        "\n",
        "Generation is smooth but factually incorrect, motivation for the attribution analysis in notebooks 02-04.\n",
        "\n",
        "Next: 02 How do the 144 attention heads (12x12) specialise?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
